{
  "version": "https://jsonfeed.org/version/1",
  "title": "Hi there!",
  "home_page_url": "http://localhost:1313/",
  "feed_url": "http://localhost:1313/feed.json",
  "description": "A minimal hugo theme focus on content",
  "favicon": "http://localhost:1313//assets/favicon.ico",
  "expired": false,
  "author": {
    "name": "Calvin Tran",
    "url": "http://localhost:1313/"
  },
  "items": [
    
    

    
    {
      "id": "85405e61444766837932b5d26b8074852544560d",
      "title": "Pytorch_internals",
      "summary": "",
      "content_text": " Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let\u0026rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them.\ntorch.compile components\nTorchDynamo: The fronend responsible for intercepting the Python code and convert it to graphs. AOTAutograd PrimTorch - It decomposes complex PyTorch operations into simpler, frequently used \u0026ldquo;primitive\u0026rdquo; operations. TorchInductor Triton(GPU), C++(CPU), CUDA (GPU) 1. TorchDynamo TorchDynamo is the first and foremost component in the sequence which is responsible for intercepting the Python code. It intercepts the Python bytecode at runtime and rewrites blocks of user code into graphs. This involves extracting subgraphs containing PyTorch operations while leaving the non-PyTorcch code untouched.\nShape Polymorphism -\nFallback - If the code can\u0026rsquo;t be converted into graphs, it safely fall backs to PyTorch\u0026rsquo;s eager execution.\nIntermediate Representation (IR) -\n2. Ahead-Of-Time Autograd (AOTAutograd) As the name suggests AOTAutograd is responsible for differentiation and backpropagation graph generation. it generates the backware computation graph (needed for the gradients and training) from the captured forward graph (passed by the TorchDynamo). However it only captures the backward graph and doesn\u0026rsquo;t apply the graph level optimization.\n3. PrimTorch It breaks down the complex PyTorch operations into simpler (primitive operations). This breaking down actually helps optimizing the graph further. It accepts a FX Graph and decomposes the operations into simpler operations.\n4. TorchInductor It further optimizes the graph and generates code to finally run on the hardware. It takes a simplified computation graphs and generates hihgly optimized low level code for the target hardware (CPU, HPU, GPU).\nIt also determines hardware level optimizations such as memory planning, tiling etc.\nDue to multiple hardware adoption, torchinductor supports multiple backend based on the target hardware.\nBackend Description Inductor Default backend: highly optimized for CPUs and GPUs Eager Runs the model without the graph capture, no optimization happens in this mode aot_eager It applies the AutoAutograd to capture the graph but doesn\u0026rsquo;t apply any further backend optimization cudagraphs Leverages CUDA Graphs for reduces CPU overhead ipex Uses Intel Extension for PyTorch for CPU-optimized execution onnxrt Uses ONNX runtime for acceleration on CPU/GPU torch_tensorrt TensorRT-backend for high-speed inference on Nvidia-GPUs tvm Uses Apache TVM compiler for cross hardware inference openvino Uses Intel OpenVINO for accelerated inference on supported Intel hardware Now if you see here the output code for the torchinductor is nothing but Python. Seems counterintuitive right? We ingest in Python code and burps out Python code but the output Python code is faster.\nNOTE (DO THIS BEFORE PUBLISHING) -\nAdd a section about how each component was used in eager mode. Explain more about CPU overhead in cudagraphs. Debugging Now that we have understood what are the components in torch.compile. How can we understand how each component is doing and which component is failing when we do torch.compile.\nThe best environment variable for debugging is TORCH_COMPILE_DEBUG=1\nTorchDynamo logging References PyTorch 2.0 Live Q\u0026amp;A Series: PT2 Profiling and Debugging torch.compile, the missing manual PyTorch Logging Debugging PyTorch Memory with Snapshot pytorch.compile docs ",
      "content_html": "\u003ch1 id=\"heading\"\u003e\u003c/h1\u003e\n\u003cp\u003eEver wondered what goes inside when you actually call torch.compile(model)?\nThere are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called \u003cem\u003eIntermediate Representation (IRs)\u003c/em\u003e (more about this later) to the next component in the sequence.\nLet\u0026rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003etorch.compile components\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTorchDynamo: The fronend responsible for intercepting the Python code and convert it to graphs.\u003c/li\u003e\n\u003cli\u003eAOTAutograd\u003c/li\u003e\n\u003cli\u003ePrimTorch - It decomposes complex PyTorch operations into simpler, frequently used \u0026ldquo;primitive\u0026rdquo; operations.\u003c/li\u003e\n\u003cli\u003eTorchInductor\u003c/li\u003e\n\u003cli\u003eTriton(GPU), C++(CPU), CUDA (GPU)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\n  src=\"./assets/pytorch_internals/components_overview.avif\"\n  alt=\"Components Overview\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch3 id=\"1-torchdynamo\"\u003e1. TorchDynamo\u003c/h3\u003e\n\u003cp\u003eTorchDynamo is the first and foremost component in the sequence which is responsible for intercepting the Python code.\nIt intercepts the Python bytecode at runtime and rewrites blocks of user code into graphs. This involves extracting subgraphs containing PyTorch operations while leaving the non-PyTorcch code untouched.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eShape Polymorphism\u003c/em\u003e -\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFallback\u003c/em\u003e - If the code can\u0026rsquo;t be converted into graphs, it safely fall backs to PyTorch\u0026rsquo;s eager execution.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eIntermediate Representation (IR)\u003c/em\u003e -\u003c/p\u003e\n\u003ch3 id=\"2-ahead-of-time-autograd-aotautograd\"\u003e2. Ahead-Of-Time Autograd (AOTAutograd)\u003c/h3\u003e\n\u003cp\u003eAs the name suggests AOTAutograd is responsible for differentiation and backpropagation graph generation.\nit generates the backware computation graph (needed for the gradients and training) from the captured forward graph (passed by the TorchDynamo). However it only captures the backward graph and doesn\u0026rsquo;t apply the graph level optimization.\u003c/p\u003e\n\u003ch3 id=\"3-primtorch\"\u003e3. PrimTorch\u003c/h3\u003e\n\u003cp\u003eIt breaks down the complex PyTorch operations into simpler (primitive operations). This breaking down actually helps optimizing the graph further. It accepts a FX Graph and decomposes the operations into simpler operations.\u003c/p\u003e\n\u003ch3 id=\"4-torchinductor\"\u003e4. TorchInductor\u003c/h3\u003e\n\u003cp\u003eIt further optimizes the graph and generates code to finally run on the hardware. It takes a simplified computation graphs and generates hihgly optimized low level code for the target hardware (CPU, HPU, GPU).\u003c/p\u003e\n\u003cp\u003eIt also determines hardware level optimizations such as memory planning, tiling etc.\u003c/p\u003e\n\u003cp\u003eDue to multiple hardware adoption, torchinductor supports multiple backend\nbased on the target hardware.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eBackend\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eInductor\u003c/td\u003e\n\u003ctd\u003eDefault backend: highly optimized for CPUs and GPUs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eEager\u003c/td\u003e\n\u003ctd\u003eRuns the model without the graph capture, no optimization happens in this mode\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eaot_eager\u003c/td\u003e\n\u003ctd\u003eIt applies the AutoAutograd to capture the graph but doesn\u0026rsquo;t apply any further backend optimization\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecudagraphs\u003c/td\u003e\n\u003ctd\u003eLeverages CUDA Graphs for reduces CPU overhead\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eipex\u003c/td\u003e\n\u003ctd\u003eUses Intel Extension for PyTorch for CPU-optimized execution\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eonnxrt\u003c/td\u003e\n\u003ctd\u003eUses ONNX runtime for acceleration on CPU/GPU\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etorch_tensorrt\u003c/td\u003e\n\u003ctd\u003eTensorRT-backend for high-speed inference on Nvidia-GPUs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etvm\u003c/td\u003e\n\u003ctd\u003eUses Apache TVM compiler for cross hardware inference\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eopenvino\u003c/td\u003e\n\u003ctd\u003eUses Intel OpenVINO for accelerated inference on supported Intel hardware\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNow if you see here the output code for the torchinductor is nothing but Python. Seems counterintuitive right? We ingest in Python code and burps out Python code but the output Python code is faster.\u003c/p\u003e\n\u003cp\u003eNOTE (DO THIS BEFORE PUBLISHING) -\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdd a section about how each component was used in eager mode.\u003c/li\u003e\n\u003cli\u003eExplain more about CPU overhead in cudagraphs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"debugging\"\u003eDebugging\u003c/h2\u003e\n\u003cp\u003eNow that we have understood what are the components in torch.compile. How can we understand how each component is doing and which component is failing when we do torch.compile.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThe best environment variable for debugging is\u003c/em\u003e\n\u003ccode\u003eTORCH_COMPILE_DEBUG=1\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"torchdynamo-logging\"\u003eTorchDynamo logging\u003c/h3\u003e\n\u003ch4 id=\"references\"\u003eReferences\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/live/1FSBurHpH_Q?si=NMWkZYZx1FaNQxYs\"\u003ePyTorch 2.0 Live Q\u0026amp;A Series: PT2 Profiling and Debugging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.ezyang.com/2024/11/ways-to-use-torch-compile/\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0\"\u003etorch.compile, the missing manual\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.pytorch.org/docs/stable/logging.html\"\u003ePyTorch Logging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://zdevito.github.io/2022/08/16/memory-snapshots.html\"\u003eDebugging PyTorch Memory with Snapshot\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.pytorch.org/docs/stable/torch.compiler.html\"\u003epytorch.compile docs\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
      "url": "http://localhost:1313/posts/pytorch_internals/",
      "date_published": "22076-22-09T712:2222:00+00:00",
      "date_modified": "22076-22-09T712:2222:00+00:00",
      "author": {
        "name": "Calvin Tran",
        "url": "http://localhost:1313/"
      }
    }
    
  ]
}