<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Liste - http://localhost:1313/">
    <title>Pytorch_internals | Hi there!</title>
    <meta name="description" content="A minimal hugo theme focus on content">
    <meta property="og:title" content="Pytorch_internals" />
<meta property="og:description" content="Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let&rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/pytorch_internals/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-07-22T14:12:54+00:00" />
<meta property="article:modified_time" content="2025-07-22T14:12:54+00:00" />


    <meta itemprop="name" content="Pytorch_internals">
<meta itemprop="description" content="Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let&rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them."><meta itemprop="datePublished" content="2025-07-22T14:12:54+00:00" />
<meta itemprop="dateModified" content="2025-07-22T14:12:54+00:00" />
<meta itemprop="wordCount" content="551">
<meta itemprop="keywords" content="" />
    
    <link rel="canonical" href="http://localhost:1313/posts/pytorch_internals/">
    <link rel="icon" href="http://localhost:1313//assets/favicon.ico">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
    <link rel="alternate" type="application/atom+xml" title="Hi there!" href="http://localhost:1313//atom.xml" />
    <link rel="alternate" type="application/json" title="Hi there!" href="http://localhost:1313//feed.json" />
    <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bricolage+Grotesque">
    
    <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 bricolage grotesque,-apple-system,BlinkMacSystemFont,segoe ui,Helvetica,Arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#fffdfa;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}header{display:flex;justify-content:space-between;align-items:center;position:relative;padding-bottom:1rem}header .burger{display:none;background:0 0;border:none;padding:0;cursor:pointer}header .burger span{display:block;width:25px;height:3px;background:#000;margin:5px 0;transition:all .3s ease}@media(max-width:768px){header .burger{display:block;z-index:2}header .nav-menu{position:fixed;top:0;right:-100%;width:100%;height:100vh;background:#fffdfa;padding:2rem;transition:.3s ease;z-index:1}header .nav-menu.active{right:0}header .nav-menu ul{flex-direction:column;align-items:center;width:100%}header .nav-menu ul li{display:block;font-size:1.2rem;border-bottom:1px dotted #000;height:50px;display:flex;justify-content:center;align-items:center}header.menu-open .burger span:first-child{transform:rotate(45deg)translate(5px,6px)}header.menu-open .burger span:nth-child(2){opacity:0}header.menu-open .burger span:last-child{transform:rotate(-45deg)translate(5px,-6px)}}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none;overflow:visible;overflow-wrap:anywhere}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}table{width:100%;border-collapse:collapse}th,td{border:1px solid #ddd;text-align:left;padding:8px}th{background-color:#f2f2f2}</style>
  
    
    
    
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "posts",
        "name": "Pytorch_internals",
        "headline": "Pytorch_internals",
        "alternativeHeadline": "",
        "description": "Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let\u0026rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them.",
        "inLanguage": "en-us",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/posts\/pytorch_internals\/"
        },
        "author" : {
            "@type": "Person",
            "name": ""
        },
        "creator" : {
            "@type": "Person",
            "name": ""
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": ""
        },
        "copyrightHolder" : "Hi there!",
        "copyrightYear" : "2025",
        "dateCreated": "2025-07-22T14:12:54.00Z",
        "datePublished": "2025-07-22T14:12:54.00Z",
        "dateModified": "2025-07-22T14:12:54.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Hi there!",
            "url": "http://localhost:1313/",
            "logo": {
                "@type": "ImageObject",
                "url": "http:\/\/localhost:1313\/assets\/favicon.ico",
                "width":"32",
                "height":"32"
            }
        },
        "image": "http://localhost:1313/assets/favicon.ico",
        "url" : "http:\/\/localhost:1313\/posts\/pytorch_internals\/",
        "wordCount" : "551",
        "genre" : [ ],
        "keywords" : [ ]
    }
    </script>
     

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const burger = document.querySelector('.burger');
            const nav = document.querySelector('.nav-menu');
            const header = document.querySelector('header');
        
            if (burger) {
                burger.addEventListener('click', () => {
                    nav.classList.toggle('active');
                    header.classList.toggle('menu-open');
                });
        
                
                document.addEventListener('click', (e) => {
                    if (!header.contains(e.target) && nav.classList.contains('active')) {
                        nav.classList.remove('active');
                        header.classList.remove('menu-open');
                    }
                });
            }
        });
    </script> 
  </head>

<body>
  <a class="skip-link" href="#main">Skip to main</a>
  <main id="main">
  <div class="content">
    
<header>
  <p style="padding: 0;margin: 0;">
    <a href="http://localhost:1313/">
      <b>Hi there!</b>
      <span class="text-stone-500 animate-blink">â–®</span>
    </a>
  </p>

  
  <button class="burger" aria-label="Toggle menu">
    <span></span>
    <span></span>
    <span></span>
  </button>
  <nav class="nav-menu">
    <ul style="padding: 0;margin: 0;">
      
      
        <li class="">
          <a href="/posts/"><span>Post</span></a>
        </li>
      
        <li class="">
          <a href="/about/"><span>About</span></a>
        </li>
      
    </ul>
  </nav>
  
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">
    <section>
      <h2 class="post">Pytorch_internals</h2>
      <h1 id="heading"></h1>
<p>Ever wondered what goes inside when you actually call torch.compile(model)?
There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called <em>Intermediate Representation (IRs)</em> (more about this later) to the next component in the sequence.
Let&rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them.</p>
<p><strong>torch.compile components</strong></p>
<ul>
<li>TorchDynamo: The fronend responsible for intercepting the Python code and convert it to graphs.</li>
<li>AOTAutograd</li>
<li>PrimTorch - It decomposes complex PyTorch operations into simpler, frequently used &ldquo;primitive&rdquo; operations.</li>
<li>TorchInductor</li>
<li>Triton(GPU), C++(CPU), CUDA (GPU)</li>
</ul>
<p><img
  src="./assets/pytorch_internals/components_overview.avif"
  alt="Components Overview"
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<h3 id="1-torchdynamo">1. TorchDynamo</h3>
<p>TorchDynamo is the first and foremost component in the sequence which is responsible for intercepting the Python code.
It intercepts the Python bytecode at runtime and rewrites blocks of user code into graphs. This involves extracting subgraphs containing PyTorch operations while leaving the non-PyTorcch code untouched.</p>
<p><em>Shape Polymorphism</em> -</p>
<p><em>Fallback</em> - If the code can&rsquo;t be converted into graphs, it safely fall backs to PyTorch&rsquo;s eager execution.</p>
<p><em>Intermediate Representation (IR)</em> -</p>
<h3 id="2-ahead-of-time-autograd-aotautograd">2. Ahead-Of-Time Autograd (AOTAutograd)</h3>
<p>As the name suggests AOTAutograd is responsible for differentiation and backpropagation graph generation.
it generates the backware computation graph (needed for the gradients and training) from the captured forward graph (passed by the TorchDynamo). However it only captures the backward graph and doesn&rsquo;t apply the graph level optimization.</p>
<h3 id="3-primtorch">3. PrimTorch</h3>
<p>It breaks down the complex PyTorch operations into simpler (primitive operations). This breaking down actually helps optimizing the graph further. It accepts a FX Graph and decomposes the operations into simpler operations.</p>
<h3 id="4-torchinductor">4. TorchInductor</h3>
<p>It further optimizes the graph and generates code to finally run on the hardware. It takes a simplified computation graphs and generates hihgly optimized low level code for the target hardware (CPU, HPU, GPU).</p>
<p>It also determines hardware level optimizations such as memory planning, tiling etc.</p>
<p>Due to multiple hardware adoption, torchinductor supports multiple backend
based on the target hardware.</p>
<table>
<thead>
<tr>
<th>Backend</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inductor</td>
<td>Default backend: highly optimized for CPUs and GPUs</td>
</tr>
<tr>
<td>Eager</td>
<td>Runs the model without the graph capture, no optimization happens in this mode</td>
</tr>
<tr>
<td>aot_eager</td>
<td>It applies the AutoAutograd to capture the graph but doesn&rsquo;t apply any further backend optimization</td>
</tr>
<tr>
<td>cudagraphs</td>
<td>Leverages CUDA Graphs for reduces CPU overhead</td>
</tr>
<tr>
<td>ipex</td>
<td>Uses Intel Extension for PyTorch for CPU-optimized execution</td>
</tr>
<tr>
<td>onnxrt</td>
<td>Uses ONNX runtime for acceleration on CPU/GPU</td>
</tr>
<tr>
<td>torch_tensorrt</td>
<td>TensorRT-backend for high-speed inference on Nvidia-GPUs</td>
</tr>
<tr>
<td>tvm</td>
<td>Uses Apache TVM compiler for cross hardware inference</td>
</tr>
<tr>
<td>openvino</td>
<td>Uses Intel OpenVINO for accelerated inference on supported Intel hardware</td>
</tr>
</tbody>
</table>
<p>Now if you see here the output code for the torchinductor is nothing but Python. Seems counterintuitive right? We ingest in Python code and burps out Python code but the output Python code is faster.</p>
<p>NOTE (DO THIS BEFORE PUBLISHING) -</p>
<ul>
<li>Add a section about how each component was used in eager mode.</li>
<li>Explain more about CPU overhead in cudagraphs.</li>
</ul>
<h2 id="debugging">Debugging</h2>
<p>Now that we have understood what are the components in torch.compile. How can we understand how each component is doing and which component is failing when we do torch.compile.</p>
<p><em>The best environment variable for debugging is</em>
<code>TORCH_COMPILE_DEBUG=1</code></p>
<h3 id="torchdynamo-logging">TorchDynamo logging</h3>
<h4 id="references">References</h4>
<ol>
<li><a href="https://www.youtube.com/live/1FSBurHpH_Q?si=NMWkZYZx1FaNQxYs">PyTorch 2.0 Live Q&amp;A Series: PT2 Profiling and Debugging</a></li>
<li><a href="https://blog.ezyang.com/2024/11/ways-to-use-torch-compile/"></a></li>
<li><a href="https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0">torch.compile, the missing manual</a></li>
<li><a href="https://docs.pytorch.org/docs/stable/logging.html">PyTorch Logging</a></li>
<li><a href="https://zdevito.github.io/2022/08/16/memory-snapshots.html">Debugging PyTorch Memory with Snapshot</a></li>
<li><a href="https://docs.pytorch.org/docs/stable/torch.compiler.html">pytorch.compile docs</a></li>
</ol>

      
      <div class="post-date">
        <span class="g time">July 22, 2025 </span> &#8729;
         
      </div>
      
    </section>
    
    
    
  </div>
</main>
</body>
</html>
